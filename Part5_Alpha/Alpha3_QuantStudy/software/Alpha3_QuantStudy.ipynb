{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d653307",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using device: cuda\n",
      "=> One batch from testloader: torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import os, math, time, copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import VGG16_quant\n",
    "from models.quant_layer import QuantConv2d, act_quantization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"=> Using device:\", device)\n",
    "\n",
    "# ----------------- CIFAR-10 -----------------\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.491, 0.482, 0.447],\n",
    "    std =[0.247, 0.243, 0.262]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "testloader  = torch.utils.data.DataLoader(\n",
    "    testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "print(\"=> One batch from testloader:\", images.shape, labels.shape)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        _, pred = out.max(1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    return acc, loss_sum / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563d5cbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint: ./results/VGG16_quant/model_best.pth.tar\n",
      "\n",
      "[Checkpoint model] Test Accuracy: 90.68% | Test Loss: 0.2997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = VGG16_quant().to(device)\n",
    "model.eval()\n",
    "\n",
    "ckpt_path = \"./results/VGG16_quant/model_best.pth.tar\"\n",
    "print(\"=> Loading checkpoint:\", ckpt_path)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state_dict[k[7:]] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "acc_full, test_loss = evaluate(model, testloader, criterion)\n",
    "print(f\"\\n[Checkpoint model] Test Accuracy: {acc_full:.2f}% | Test Loss: {test_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc29b1fa-8ab2-49a3-86cb-7ef2e9445c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Number of QuantConv2d layers: 13\n",
      "  Layer 0: in=3, out=64, k=(3, 3)\n",
      "  Layer 1: in=64, out=64, k=(3, 3)\n",
      "  Layer 2: in=64, out=128, k=(3, 3)\n",
      "  Layer 3: in=128, out=128, k=(3, 3)\n",
      "  Layer 4: in=128, out=256, k=(3, 3)\n",
      "  Layer 5: in=256, out=256, k=(3, 3)\n",
      "  Layer 6: in=256, out=256, k=(3, 3)\n",
      "  Layer 7: in=256, out=512, k=(3, 3)\n",
      "  Layer 8: in=512, out=512, k=(3, 3)\n",
      "  Layer 9: in=512, out=512, k=(3, 3)\n",
      "  Layer 10: in=512, out=512, k=(3, 3)\n",
      "  Layer 11: in=512, out=512, k=(3, 3)\n",
      "  Layer 12: in=512, out=512, k=(3, 3)\n"
     ]
    }
   ],
   "source": [
    "qconvs = [m for m in model.modules() if isinstance(m, QuantConv2d)]\n",
    "print(\"=> Number of QuantConv2d layers:\", len(qconvs))\n",
    "\n",
    "for idx, m in enumerate(qconvs):\n",
    "    print(f\"  Layer {idx}: in={m.in_channels}, out={m.out_channels}, k={m.kernel_size}\")\n",
    "\n",
    "for m in qconvs:\n",
    "    m.act_bit = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0cd5b7-9447-4432-ba94-316dfa182c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_all_act_bits(model, bit: int):\n",
    "    \"\"\"\n",
    "    Change the activation bit of all QuantConv2d quantizations in the model to the same value (2 or 4).\n",
    "    This only affects activation quantization, not weight quantization.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, QuantConv2d):\n",
    "            m.act_bit = bit\n",
    "            m.act_alq = act_quantization(bit)  # act_quantization(b) 返回一个新的量化函数\n",
    "\n",
    "\n",
    "def set_layer_act_bit(model, layer_idx: int, bit: int):\n",
    "    \"\"\"\n",
    "    Only modify the activation bit of the layer_idx QuantConv2d.\n",
    "    \"\"\"\n",
    "    qconvs_local = [m for m in model.modules() if isinstance(m, QuantConv2d)]\n",
    "    m = qconvs_local[layer_idx]\n",
    "    m.act_bit = bit\n",
    "    m.act_alq = act_quantization(bit)\n",
    "\n",
    "\n",
    "def eval_acc(model):\n",
    "    acc, _ = evaluate(model, testloader, criterion)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233b8bfb-d5ad-4d07-acc6-cc3a6ed06d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[All-4bit activation] Test Accuracy: 90.68%\n"
     ]
    }
   ],
   "source": [
    "set_all_act_bits(model, 4)\n",
    "\n",
    "acc_all4 = eval_acc(model)\n",
    "print(f\"[All-4bit activation] Test Accuracy: {acc_all4:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d126410a-ea0d-48b6-be82-56ac17b52f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Running per-layer sensitivity analysis ...\n",
      "  [Layer  0 -> 2bit] acc=81.18%, drop=9.50%\n",
      "  [Layer  1 -> 2bit] acc=89.53%, drop=1.15%\n",
      "  [Layer  2 -> 2bit] acc=89.82%, drop=0.86%\n",
      "  [Layer  3 -> 2bit] acc=89.97%, drop=0.71%\n",
      "  [Layer  4 -> 2bit] acc=89.68%, drop=1.00%\n",
      "  [Layer  5 -> 2bit] acc=89.31%, drop=1.37%\n",
      "  [Layer  6 -> 2bit] acc=90.14%, drop=0.54%\n",
      "  [Layer  7 -> 2bit] acc=90.04%, drop=0.64%\n",
      "  [Layer  8 -> 2bit] acc=90.58%, drop=0.10%\n",
      "  [Layer  9 -> 2bit] acc=90.74%, drop=-0.06%\n",
      "  [Layer 10 -> 2bit] acc=90.52%, drop=0.16%\n",
      "  [Layer 11 -> 2bit] acc=90.56%, drop=0.12%\n",
      "  [Layer 12 -> 2bit] acc=90.56%, drop=0.12%\n",
      "\n",
      "=> Per-layer sensitivity sorted by accuracy drop:\n",
      "  Layer  9: acc=90.74%, drop=-0.06%\n",
      "  Layer  8: acc=90.58%, drop=0.10%\n",
      "  Layer 11: acc=90.56%, drop=0.12%\n",
      "  Layer 12: acc=90.56%, drop=0.12%\n",
      "  Layer 10: acc=90.52%, drop=0.16%\n",
      "  Layer  6: acc=90.14%, drop=0.54%\n",
      "  Layer  7: acc=90.04%, drop=0.64%\n",
      "  Layer  3: acc=89.97%, drop=0.71%\n",
      "  Layer  2: acc=89.82%, drop=0.86%\n",
      "  Layer  4: acc=89.68%, drop=1.00%\n",
      "  Layer  1: acc=89.53%, drop=1.15%\n",
      "  Layer  5: acc=89.31%, drop=1.37%\n",
      "  Layer  0: acc=81.18%, drop=9.50%\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(qconvs)\n",
    "sensitivity_results = []\n",
    "\n",
    "print(\"=> Running per-layer sensitivity analysis ...\")\n",
    "\n",
    "for idx in range(num_layers):\n",
    "    set_all_act_bits(model, 4)\n",
    "\n",
    "    set_layer_act_bit(model, idx, 2)\n",
    "\n",
    "    acc = eval_acc(model)\n",
    "    acc_drop = (acc_all4 - acc) / 100.0\n",
    "\n",
    "    sensitivity_results.append({\n",
    "        \"layer\": idx,\n",
    "        \"acc\": acc,\n",
    "        \"acc_drop\": acc_all4 - acc,\n",
    "    })\n",
    "    print(f\"  [Layer {idx:2d} -> 2bit] acc={acc:.2f}%, drop={acc_all4 - acc:.2f}%\")\n",
    "\n",
    "sensitivity_sorted = sorted(sensitivity_results, key=lambda x: x[\"acc_drop\"])\n",
    "\n",
    "print(\"\\n=> Per-layer sensitivity sorted by accuracy drop:\")\n",
    "for r in sensitivity_sorted:\n",
    "    print(f\"  Layer {r['layer']:2d}: acc={r['acc']:.2f}%, drop={r['acc_drop']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd3e5b0-bde0-4899-b79c-5ca4608e62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Start greedy search from all-4bit baseline: acc=90.68%, epsilon=1.00%\n",
      "\n",
      "  ACCEPT layer  9 -> 2bit | acc=90.74%, drop=-0.06%\n",
      "  ACCEPT layer  8 -> 2bit | acc=90.62%, drop=0.06%\n",
      "  ACCEPT layer 11 -> 2bit | acc=90.30%, drop=0.38%\n",
      "  ACCEPT layer 12 -> 2bit | acc=90.22%, drop=0.46%\n",
      "  ACCEPT layer 10 -> 2bit | acc=90.35%, drop=0.33%\n",
      "  REJECT layer  6 -> 2bit | acc=89.32%, drop=1.36%  (> 1.00%)\n",
      "  REJECT layer  7 -> 2bit | acc=89.41%, drop=1.27%  (> 1.00%)\n",
      "  REJECT layer  3 -> 2bit | acc=89.65%, drop=1.03%  (> 1.00%)\n",
      "  REJECT layer  2 -> 2bit | acc=89.21%, drop=1.47%  (> 1.00%)\n",
      "  REJECT layer  4 -> 2bit | acc=89.00%, drop=1.68%  (> 1.00%)\n",
      "  REJECT layer  1 -> 2bit | acc=89.06%, drop=1.62%  (> 1.00%)\n",
      "  REJECT layer  5 -> 2bit | acc=88.88%, drop=1.80%  (> 1.00%)\n",
      "  REJECT layer  0 -> 2bit | acc=80.42%, drop=10.26%  (> 1.00%)\n",
      "\n",
      "==================== Final Mixed-Precision Config ====================\n",
      "Per-layer activation bits (per QuantConv2d):\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2]\n",
      "\n",
      "Total conv layers = 13\n",
      "2bit layers       = 5 (38.5%)\n",
      "\n",
      "Final mixed-precision accuracy     : 90.35%\n",
      "Accuracy drop vs all-4bit baseline : 0.33%\n"
     ]
    }
   ],
   "source": [
    "# maximum allowed accuracy drop\n",
    "epsilon = 1.0\n",
    "\n",
    "num_layers = len(qconvs)\n",
    "precisions = [4] * num_layers\n",
    "\n",
    "set_all_act_bits(model, 4)\n",
    "current_acc = acc_all4\n",
    "\n",
    "print(f\"=> Start greedy search from all-4bit baseline: acc={acc_all4:.2f}%, epsilon={epsilon:.2f}%\\n\")\n",
    "\n",
    "for r in sensitivity_sorted:\n",
    "    idx = r[\"layer\"]\n",
    "\n",
    "    set_layer_act_bit(model, idx, 2)\n",
    "    precisions[idx] = 2\n",
    "\n",
    "    acc = eval_acc(model)\n",
    "    drop = acc_all4 - acc\n",
    "\n",
    "    if drop <= epsilon:\n",
    "        current_acc = acc\n",
    "        print(f\"  ACCEPT layer {idx:2d} -> 2bit | acc={acc:.2f}%, drop={drop:.2f}%\")\n",
    "    else:\n",
    "        set_layer_act_bit(model, idx, 4)\n",
    "        precisions[idx] = 4\n",
    "        print(f\"  REJECT layer {idx:2d} -> 2bit | acc={acc:.2f}%, drop={drop:.2f}%  (> {epsilon:.2f}%)\")\n",
    "\n",
    "print(\"\\n==================== Final Mixed-Precision Config ====================\")\n",
    "print(\"Per-layer activation bits (per QuantConv2d):\")\n",
    "print(precisions)\n",
    "\n",
    "num_2bit = sum(1 for b in precisions if b == 2)\n",
    "print(f\"\\nTotal conv layers = {num_layers}\")\n",
    "print(f\"2bit layers       = {num_2bit} ({num_2bit/num_layers*100:.1f}%)\")\n",
    "print(f\"\\nFinal mixed-precision accuracy     : {current_acc:.2f}%\")\n",
    "print(f\"Accuracy drop vs all-4bit baseline : {acc_all4 - current_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900d6aa-b62c-4e7c-96ef-ae3d7bc443fb",
   "metadata": {},
   "source": [
    "### Mixed-Precision Study (2-bit / 4-bit)\n",
    "\n",
    "Here we perform a per-layer mixed-precision study on the `VGG16_quant` model (CIFAR-10), which:\n",
    "\n",
    "- We keep weight quantization unchanged\n",
    "- We only change activation bit-width of each `QuantConv2d` layer between 4-bit and 2-bit.\n",
    "- The baseline is all layers with 4-bit activations.\n",
    "\n",
    "Baseline (all 4-bit activations)\n",
    "- Test accuracy: **90.68%**\n",
    "\n",
    "\n",
    "### Per-layer Sensitivity & Greedy Algorithm\n",
    "\n",
    "1. Per-layer sensitivity\n",
    "   - For each of the 13 `QuantConv2d` layers:\n",
    "     - Set all layers to 4-bit.\n",
    "     - Change only that layer to 2-bit.\n",
    "     - Measure the new accuracy and record the accuracy drop vs. the all-4bit baseline.\n",
    "   - This tells us how sensitive each layer is to reducing its activation precision from 4-bit to 2-bit.\n",
    "\n",
    "2. Greedy mixed-precision search\n",
    "   - Start from all-4bit configuration.\n",
    "   - Sort layers by their individual accuracy drop (least sensitive → most sensitive).\n",
    "   - Traverse this ordered list; for each candidate layer:\n",
    "     - Tentatively switch it from 4-bit → 2-bit.\n",
    "     - Re-evaluate accuracy with all previously accepted 2-bit layers included.\n",
    "     - Accept the change if the total accuracy drop (vs. all-4bit) is ≤ ε (here ε = 1.0%); otherwise reject it.\n",
    "\n",
    "\n",
    "### Final Mixed-Precision Configuration\n",
    "\n",
    "The final per-layer activation bit-widths (over 13 convolution layers) are:\n",
    "\n",
    "```text\n",
    "[4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b48746-992f-4729-bd76-49e1b446feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcca837-2a55-4737-b821-dfffc1c52f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
